{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Undersample data\n",
    "    - Undersample raw data \n",
    "    - Undersample cleaned data\n",
    "1. LogReg with raw data (imbalanced)\n",
    "2. LogReg with data cleaned of extreme values (imbalanced)\n",
    "5. GNB with undersampled raw data (balanced)\n",
    "6. GNB with undersampled cleaned data (balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing: \n",
    "1. Stratified split train-test \n",
    "2. Transform training data \n",
    "    - log(amount+0.01)\n",
    "    - clean extreme values \n",
    "    - scale all data feature using robust scaler \n",
    "3. Feature selection \n",
    "    - drop time \n",
    "    - drop features whose distributions of class 0 & class 1 are similar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "import collections\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,auc,roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_by_class(data):\n",
    "    '''\n",
    "    separate the data set into fraud transactions (Class 1) and genuine transaction (Class 0)\n",
    "\n",
    "    input: \n",
    "        data: entire data set \n",
    "    output: \n",
    "        a set of genuine transactions\n",
    "        a set of fraud transactions \n",
    "        (in that order) \n",
    "    '''\n",
    "    fraud = data[data.Class==1]\n",
    "    not_fraud = data[data.Class==0]\n",
    "\n",
    "    return fraud, not_fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_count_bin(data, col):\n",
    "    '''\n",
    "    made for testing, we dont actually have to use this\n",
    "    '''\n",
    "    counts, bin_edges = np.histogram(data[col], bins=10)\n",
    "    for i in range(len(counts)):\n",
    "        print(f\"Between {round(bin_edges[i],1)} and {round(bin_edges[i+1],1)}: \\t\", end='')\n",
    "        print(f\"{counts[i]} ({round(100*counts[i]/len(not_fraud),2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_extreme_amount(data, col='Amount', threshold=5000):\n",
    "    '''\n",
    "    eliminate any transactions with Amount higher than a specific threshold. \n",
    "    default threshold is $5000\n",
    "\n",
    "    input: \n",
    "        data: entire data set \n",
    "        col: default 'Amount'\n",
    "        threshold: default to be $5000\n",
    "    output: \n",
    "        new data set that only have entries with values <= threshold \n",
    "    '''\n",
    "    new_data = data[(data[col] <= threshold)].copy()\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_train_test_byclass(data, feature_names, by, test_size=0.3):\n",
    "    '''\n",
    "    split train & test set using stratified split (not random split) so as to avoid \n",
    "    distribution shift \n",
    "    \n",
    "    input: \n",
    "        data: entire data set \n",
    "        feature_names: features that we care about \n",
    "        by: list of features that we want to stratified split by \n",
    "        test_size: percentage of test set (default to be 0.3)\n",
    "    output: \n",
    "        X_train: set of training observations \n",
    "        X_test: set of test observations \n",
    "        y_train: label for training set \n",
    "        y_test: label for test set\n",
    "    '''\n",
    "    X = data[feature_names]\n",
    "    y = data[class_name]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=test_size, random_state=1, stratify=data[by])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(clf, X_train, y_train, X_test): \n",
    "    '''\n",
    "    train model according to the specified classifier\n",
    "    \n",
    "    input: \n",
    "        clf: classifier (as imported from sklearn module)\n",
    "        X_train: training observations \n",
    "        y_train: label for training observations \n",
    "        X_test: test observations \n",
    "    output: \n",
    "        y_predict: predicted label for the test observations \n",
    "        y_predict_proba: predicted class probability for the test observations\n",
    "    '''\n",
    "    # create classifier\n",
    "    clf = clf \n",
    "    # fit classifier to training data \n",
    "    clf.fit(X_train, y_train)\n",
    "    # predict on test data \n",
    "    y_predict = clf.predict(X_test)\n",
    "    # compute predicted probability \n",
    "    y_predict_proba = clf.predict_proba(X_test)\n",
    "    return y_predict, y_predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(y_test, y_predict, y_predict_proba):\n",
    "    '''\n",
    "    print the scores\n",
    "    '''\n",
    "    print(\"test set confusion matrix:\\n\", confusion_matrix(y_test, y_predict))\n",
    "    print(\"recall score: \", recall_score(y_test, y_predict))\n",
    "    print(\"precision score: \", precision_score(y_test,y_predict))\n",
    "    print(\"accuracy score: \", accuracy_score(y_test, y_predict))\n",
    "    print(\"f1 score: \", f1_score(y_test,y_predict))\n",
    "    print(\"ROC AUC: {}\".format(roc_auc_score(y_test, y_predict_proba[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split_data(df, drop_list=[]):\n",
    "    df = df.drop(drop_list,axis=1)\n",
    "    print(df.columns)\n",
    "    #test train split time\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    y = df['Class'].values #target\n",
    "    X = df.drop(['Class'],axis=1).values #features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=42, stratify=y)\n",
    "\n",
    "    print(\"train-set size: \", len(y_train),\n",
    "      \"\\ntest-set size: \", len(y_test))\n",
    "    print(\"fraud cases in test-set: \", sum(y_test))\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into Train & Test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pj_path = os.getcwd() ## get current path\n",
    "data_path = os.path.join(pj_path, 'creditcard.csv')\n",
    "\n",
    "df = pd.read_csv(data_path) # load data from the given csv file\n",
    "\n",
    "## get feature and class names\n",
    "feature_names = df.columns[:-1]\n",
    "class_name = df.columns[-1]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate some features in which Fraud and Not Fraud have similar dist (mostly normal)\n",
    "not_used = ['V15', 'V20', 'V22', 'V24', 'V25', 'V26', 'V28']\n",
    "feature_names = np.setdiff1d(feature_names, not_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genuine transactions shape:  (230628, 31)\n",
      "Fraud transactions shape:  (492, 31)\n",
      "Trimmed data shape:  (231120, 31)\n"
     ]
    }
   ],
   "source": [
    "fraud, not_fraud = get_data_by_class(df)\n",
    "\n",
    "Q1 = not_fraud[np.setdiff1d(feature_names, ['Time', 'Amount', 'V1', 'TimeHour'])].quantile(0.25)\n",
    "Q3 = not_fraud[np.setdiff1d(feature_names, ['Time', 'Amount', 'V1', 'TimeHour'])].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_whisker = Q1 - 2.5 * IQR\n",
    "higher_whisker = Q3 + 2.5 * IQR\n",
    "\n",
    "not_fraud = not_fraud[~((not_fraud < lower_whisker) |(not_fraud > higher_whisker)).any(axis=1)]\n",
    "print(\"Genuine transactions shape: \", not_fraud.shape)\n",
    "print(\"Fraud transactions shape: \", fraud.shape)\n",
    "\n",
    "trimmed_data = pd.concat([fraud, not_fraud]).reset_index(drop=True)\n",
    "print(\"Trimmed data shape: \", trimmed_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert seconds to hour, assign this info to a new column\n",
    "to_hour = np.floor(trimmed_data.Time/(60*60)).astype(int)\n",
    "for t in range(len(to_hour)):\n",
    "    if to_hour[t] >= 24:\n",
    "        to_hour[t] = to_hour[t] - 24\n",
    "\n",
    "trimmed_data['TimeHour'] = to_hour\n",
    "\n",
    "## eliminate data with Amount > 3000\n",
    "trimmed_data = eliminate_extreme_amount(trimmed_data, threshold=3000)\n",
    "\n",
    "## log(Amount)\n",
    "trimmed_data = trimmed_data[trimmed_data.Amount != 0].reset_index(drop=True) ## it drops some fraud \n",
    "trimmed_data['lnAmount'] = np.log(trimmed_data['Amount'])\n",
    "\n",
    "## split train-test sets using stratified split with respect to Class (Fraud or Non-fraud) and Time (24 hours a day)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    stratified_train_test_byclass(trimmed_data, feature_names, [class_name, 'TimeHour'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_fraud_index = np.array(df[df.Class==1].index)\n",
    "n_fraud = len(full_fraud_index)\n",
    "n_fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing 27 fraud transactions with amount=0\n",
    "len(trimmed_data[trimmed_data.Class==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions in full undersampled data:  984\n",
      "Ratio between Class 0 & Class 1 in full undersampled data:  1.0\n"
     ]
    }
   ],
   "source": [
    "# RAW DATA\n",
    "full_genuine_index = df[df.Class==0].index\n",
    "\n",
    "full_rus_genuine_index = np.array(np.random.choice(a=full_genuine_index, size=n_fraud, replace=False))\n",
    "full_rus_index = np.concatenate([full_fraud_index, full_rus_genuine_index])\n",
    "full_undersample_df = df.iloc[full_rus_index, :]\n",
    "\n",
    "full_y_undersample = full_undersample_df['Class'].values\n",
    "full_X_undersample = full_undersample_df.drop(['Class'], axis=1).values\n",
    "\n",
    "print(\"Number of transactions in full undersampled data: \", len(full_undersample_df))\n",
    "print(\"Ratio between Class 0 & Class 1 in full undersampled data: \", len(full_undersample_df[full_undersample_df.Class==0])/len(full_undersample_df[full_undersample_df.Class==1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TRIMMED DATA\n",
    "# trimmed_genuine_index = trimmed_data[trimmed_data.Class==0].index\n",
    "\n",
    "# trimmed_rus_genuine_index = np.array(np.random.choice(a=trimmed_genuine_index, size=n_fraud, replace=False))\n",
    "# trimmed_rus_index = np.concatenate([full_fraud_index, trimmed_rus_genuine_index])\n",
    "# trimmed_undersample_df = trimmed_data.iloc[trimmed_rus_index, :]\n",
    "\n",
    "# trimmed_undersample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LogReg assumes the dependent variable is binary, as is the case of this problem where we have 2 labels: 0 (Genuine) and 1 (Fraud). \n",
    "- LogReg assumes data is linear separable. As we have no idea how to verify this (it is part of the truth of the data), we ignore this ^^ \n",
    "- LogReg assumes no highly influential outlier data. Thus, we shall clean the data set of extreme values to enhance model's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On raw data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n",
      "train-set size:  199364 \n",
      "test-set size:  85443\n",
      "fraud cases in test-set:  148\n"
     ]
    }
   ],
   "source": [
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = random_split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION ON RAW DATA RESULTS\n",
      "test set confusion matrix:\n",
      " [[85280    15]\n",
      " [   68    80]]\n",
      "recall score:  0.5405405405405406\n",
      "precision score:  0.8421052631578947\n",
      "accuracy score:  0.9990285921608558\n",
      "f1 score:  0.6584362139917695\n",
      "ROC AUC: 0.9380979050449711\n"
     ]
    }
   ],
   "source": [
    "y_raw_LRpred, y_raw_LRpred_proba = get_predictions(LogisticRegression(C=0.01, penalty='l1', solver='liblinear'),\n",
    "                                                                X_train_raw, y_train_raw, X_test_raw)\n",
    "\n",
    "print('LOGISTIC REGRESSION ON RAW DATA RESULTS')\n",
    "print_scores(y_test_raw, y_raw_LRpred, y_raw_LRpred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On cleaned data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION ON CLEANED DATA RESULTS\n",
      "test set confusion matrix:\n",
      " [[68754     0]\n",
      " [   31   106]]\n",
      "recall score:  0.7737226277372263\n",
      "precision score:  1.0\n",
      "accuracy score:  0.9995500137898999\n",
      "f1 score:  0.8724279835390947\n",
      "ROC AUC: 0.9492360258694438\n"
     ]
    }
   ],
   "source": [
    "y_cleaned_LRpred, y_cleaned_LRpred_proba = get_predictions(LogisticRegression(C=0.01, penalty='l1', solver='liblinear'),\n",
    "                                                                X_train, y_train, X_test)\n",
    "print('LOGISTIC REGRESSION ON CLEANED DATA RESULTS')\n",
    "print_scores(y_test, y_cleaned_LRpred, y_cleaned_LRpred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On undersampled data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n",
      "train-set size:  688 \n",
      "test-set size:  296\n",
      "fraud cases in test-set:  148\n"
     ]
    }
   ],
   "source": [
    "X_und_train, X_und_test, y_und_train, y_und_test = \\\n",
    "random_split_data(full_undersample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION ON UNDERSAMPLED DATA RESULTS\n",
      "test set confusion matrix:\n",
      " [[145   3]\n",
      " [ 19 129]]\n",
      "recall score:  0.8716216216216216\n",
      "precision score:  0.9772727272727273\n",
      "accuracy score:  0.9256756756756757\n",
      "f1 score:  0.9214285714285714\n",
      "ROC AUC: 0.9639791818845872\n"
     ]
    }
   ],
   "source": [
    "y_und_LRpred, y_und_LRpred_proba = get_predictions(LogisticRegression(C=0.01, penalty='l1', solver='liblinear'),\n",
    "                                                   X_und_train, y_und_train, X_und_test)\n",
    "\n",
    "print('LOGISTIC REGRESSION ON UNDERSAMPLED DATA RESULTS')\n",
    "print_scores(y_und_test, y_und_LRpred, y_und_LRpred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayesian "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GNB assumes strong independence between features. However, in real life we don't usually have perfect independence, so we proceed anyways :) This assumption is why the model is named \"Naive\". \n",
    "- GNB assumes the distributions of classes in each feature follows a Gaussian distribution --> this seems like a far reach for out data, because often we don't have normal distribution. But this is the best we can do for now :) \n",
    "- From the above formula, we can tell that a heavily imbalanced data set like this will badly influence GNB's prediction as a blind guess of Class 0 will yields accuracy >= 99% (but this is useless for fraud detection purposes) --> need balancing data \n",
    "    - Undersampling is preferred because oversampling from 500 observations to 23k observations means 99% fraud observations are synthetic data --> sus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "p(y|data) = \\frac{p(data|y)*p(y)}{p(data)} \\\\\n",
    "\n",
    "p(y|data) \\propto p(data|y)*p(y)\n",
    "\\end{align}\n",
    "\n",
    "Assuming all features $a_1, a_2,..., a_T$ of data are IID: \n",
    "\\begin{align}\n",
    "p(data|y) = p(a_1,..., a_T|y) = \\prod_{i=1}^T p(a_i|y)\n",
    "\\end{align}\n",
    "\n",
    "To improve model simplicity, we can eliminate features that have similar distributions between Class 0 and Class 1. In other words, if $p(X_i|y1)$ ~ $p(X_i|y2)$, we can eliminate $X_i$ from the model because: \n",
    "\n",
    "\\begin{align}\n",
    "p(y1|data) \\propto p(data|y1)*p(y1) = [\\prod_{i=1}^T p(x_i|y1)] * p(y1) \\\\ \n",
    "\n",
    "p(y2|data) \\propto p(data|y2)*p(y2) = [\\prod_{i=1}^T p(x_i|y2)] * p(y2) \n",
    "\\end{align}\n",
    "If $p(X_i|y1)$ ~ $p(X_i|y2)$, then the following ratio stays relatively the same. \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{p(y1|data)}{p(y2|data)} = \\frac{[\\prod_{i=1}^T p(x_i|y1)] * p(y1)}{[\\prod_{i=1}^T p(x_i|y2)] * p(y2)}\n",
    "\\end{align}\n",
    "\n",
    "Or, the final classificaion is not much influenced by the removal of class $X_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On undersampled data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAUSSIAN NAIVE BAYES ON UNDERSAMPLED DATA RESULTS\n",
      "test set confusion matrix:\n",
      " [[145   3]\n",
      " [ 44 104]]\n",
      "recall score:  0.7027027027027027\n",
      "precision score:  0.9719626168224299\n",
      "accuracy score:  0.8412162162162162\n",
      "f1 score:  0.815686274509804\n",
      "ROC AUC: 0.9693206720233747\n"
     ]
    }
   ],
   "source": [
    "y_und_GNBpred, y_und_GNBpred_proba = get_predictions(GaussianNB(), X_und_train, y_und_train, X_und_test)\n",
    "\n",
    "print('GAUSSIAN NAIVE BAYES ON UNDERSAMPLED DATA RESULTS')\n",
    "print_scores(y_und_test, y_und_GNBpred, y_und_GNBpred_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
